{
 "metadata": {
  "name": "",
  "signature": "sha256:c0eb1205ca0c53978f7dc7c19eb1adbdf9c57bb985f037c647f35339d9ae4da7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from numpy.linalg import norm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Stability of least squares algorithms"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's set up a least squares problem.  We'll fit a polynomial of degree 14 to the function \n",
      "\n",
      "$$e^{\\sin(4t)}.$$\n",
      "\n",
      "We'll normalize the function (based on knowledge of the exact answer) so that the correct answer for the leading coefficient of the interpolating polynomial is 1."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = 100  # Sample at this many points\n",
      "n = 15   # Fit a polynomial of degree n-1\n",
      "\n",
      "t = np.linspace(0,1,m)\n",
      "A = np.zeros((m,n))\n",
      "for i in range(n):\n",
      "    A[:,i] = t**i      # Vandermonde matrix\n",
      "    \n",
      "b = np.exp(np.sin(4*t))  # Sampled function\n",
      "b = b/2006.787453080206  # Normalization"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's determine the least squares condition number for this problem.  Note that in order to do so, we have to actually solve the problem.  We also plot the fit, which is very close."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x, res, rank, s = np.linalg.lstsq(A,b)\n",
      "y = np.dot(A,x)\n",
      "kappa = np.linalg.cond(A)\n",
      "theta = np.arcsin(norm(b-y)/norm(b))\n",
      "eta = norm(A)*norm(x)/norm(y)\n",
      "print 'kappa: %0.2e' % kappa\n",
      "print 'theta: %0.2e' % theta\n",
      "print 'eta: %10.2e' % eta\n",
      "print 'kappa^2 * tan(theta)/eta: %10.2e' % term2\n",
      "\n",
      "plt.plot(t,b,'-',t,y,'o');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that the matrix is very ill-conditioned (because of the high polynomial degree), but $\\theta$ is very small which means we can fit the data very well.  Recall our formula for the condition number:\n",
      "\n",
      "$$ \\kappa(A) + \\frac{\\kappa(A)^2 \\tan(\\theta)}{\\eta}. $$\n",
      "\n",
      "In this case, because $\\theta$ is small and $\\eta$ is fairly large, both terms are comparable.  Thus we get a condition number of about $\\kappa$, rather than $\\kappa^2$.  This is often the case when the problem is ill-conditioned and the fit is close."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'condition number: %0.2e' % (kappa + kappa**2*np.tan(theta)/eta)\n",
      "print 'kappa^2: %0.2e' % kappa**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on this, we should hope to get perhaps 6 digits of accuracy from a backward-stable algorithm (in double precision).  Meanwhile, the error from using the normal equations will be governed by $\\kappa^2$, which is much larger."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Solution methods and accuracy"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Numpy QR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, let's try numpy's built-in QR factorization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q, R = np.linalg.qr(A)\n",
      "x = np.linalg.solve(R,np.dot(Q.T,b))\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does a bit better than what the analysis guarantees.  It seems this uses a backward-stable algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Householder QR"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's try using our own implementation of Householder factorization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def householder(A):\n",
      "    \"\"\"QR factorization via Householder triangularization.\"\"\"\n",
      "    m, n = A.shape\n",
      "    V = np.zeros(A.shape)\n",
      "    R = A.copy()\n",
      "    for k in range(n-1):\n",
      "        x = R[k:,k].copy()\n",
      "        x[0] = x[0] + np.sign(x[0])*np.linalg.norm(x,2)\n",
      "        x = x/np.linalg.norm(x,2)\n",
      "        V[k:,k] = x.copy()\n",
      "        for j in range(k,n):\n",
      "            R[k:,j] = R[k:,j] - 2*V[k:,k]*np.dot(V[k:,k].T,R[k:,j])\n",
      "    return V,R[:n,:]\n",
      "\n",
      "def apply_Q(V,x):\n",
      "    \"\"\"Algorithm 10.3 of Trefethen & Bau.\"\"\"\n",
      "    m, n = V.shape\n",
      "    for k in range(n-1,-1,-1):\n",
      "        x[k:] = x[k:] - 2*np.dot(V[k:,k],x[k:])*V[k:,k]\n",
      "    return x\n",
      "\n",
      "def compute_Q(V):\n",
      "    m, n = V.shape\n",
      "    Q = np.zeros((m,n))\n",
      "    for k in range(n):\n",
      "        x = np.zeros(m)\n",
      "        x[k] = 1.\n",
      "        Q[:,k] = apply_Q(V,x)\n",
      "    return Q"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "V, R = householder(A)\n",
      "Q = compute_Q(V)\n",
      "x = np.linalg.solve(R,np.dot(Q.T,b))\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What happened here?  Even though Householder is backward stable, my method for applying $Q^T$ is not.  However, there is a sneaky way to avoid this problem.  We just append $b$ to $A$ as a final column:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.append(A,b.reshape((m,1)),axis=1)\n",
      "V, R = householder(M)\n",
      "R2 = R[:n,:n]\n",
      "Qb = R[:n,n]\n",
      "x = np.linalg.solve(R2,Qb)\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we get the expected accuracy -- almost as good as numpy's QR."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Modified Gram-Schmidt"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's try Modified Gram-Schmidt.  We already know this is an unstable algorithm for computing $Q$, but here goes..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mgs(A):\n",
      "    \"QR factorization by the modified Gram-Schmidt algorithm.\"\n",
      "    n = A.shape[1]\n",
      "    R = np.zeros([n,n])\n",
      "    V = np.zeros(A.shape)\n",
      "    Q = np.zeros(A.shape)\n",
      "    for i in range(n):\n",
      "        V[:,i] = A[:,i]\n",
      "    for i in range(n):\n",
      "        R[i,i] = np.linalg.norm(V[:,i],2)\n",
      "        Q[:,i] = V[:,i]/R[i,i]\n",
      "        for j in range(i,n):\n",
      "            R[i,j] = np.dot(Q[:,i].T,V[:,j])\n",
      "            V[:,j] = V[:,j] - R[i,j]*Q[:,i]\n",
      "    return Q, R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q, R = mgs(A)\n",
      "x = np.linalg.solve(R,np.dot(Q.T,b))\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since the $Q$ matrix we get from MGS is not orthogonal, then $Q^T$ is not its inverse, and the algorithm fails.  There is again a sneaky way to get around this and avoid the need to use $Q^T$ explicitly:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M = np.append(A,b.reshape((m,1)),axis=1)\n",
      "Q2, R = mgs(M)\n",
      "R2 = R[:n,:n]\n",
      "Qb = R[:n,n]\n",
      "x = np.linalg.solve(R2,Qb)\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implemented this way, MGS is just as good as the other algorithms we've tried."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Normal equations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about the normal equations?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = np.linalg.solve(np.dot(A.T,A),np.dot(A.T,b))\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since $\\kappa^2 \\approx 10^{20}$, we don't get a single accurate digit from this algorithm."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, let's try the SVD:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U, S, Vstar = np.linalg.svd(A)\n",
      "x = np.dot(Vstar.T,np.dot(U[:,:n].T,b)/S)\n",
      "print x[14]-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is almost exactly the same accuracy as we got from numpy's built-in QR function."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lower-degree fitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try fitting a more-oscillatory function with just a quadratic."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = 100  # Sample at this many points\n",
      "n = 4   # Fit a polynomial of degree n-1\n",
      "\n",
      "t = np.linspace(0,1,m)\n",
      "A = np.zeros((m,n))\n",
      "for i in range(n):\n",
      "    A[:,i] = t**i      # Vandermonde matrix\n",
      "    \n",
      "b = np.exp(np.sin(40*t))  # Sampled function\n",
      "\n",
      "x, res, rank, s = np.linalg.lstsq(A,b)\n",
      "y = np.dot(A,x)\n",
      "kappa = np.linalg.cond(A)\n",
      "theta = np.arcsin(norm(b-y)/norm(b))\n",
      "eta = norm(A)*norm(x)/norm(y)\n",
      "term2 = kappa**2*np.tan(theta)/eta\n",
      "rat = np.tan(theta)/eta\n",
      "print 'kappa: %0.2e' % kappa\n",
      "print 'theta: %0.2e' % theta\n",
      "print 'eta: %10.2e' % eta\n",
      "print 'kappa^2 * tan(theta)/eta: %10.2e' % term2\n",
      "print 'kappa^2: %0.2e' % kappa**2\n",
      "plt.plot(t,b,'-',t,y,'o');\n",
      "\n",
      "# Use sympy to get \"exact\" solution\n",
      "import sympy\n",
      "def f(i,j):\n",
      "    return sympy.Rational(i,m-1)**j\n",
      "AA=sympy.Matrix(m,n,f)\n",
      "bb = sympy.Matrix(b).T\n",
      "x_ex=AA.LDLsolve(bb)\n",
      "\n",
      "# Note that we don't bother to normalize this time because x_ex[-1] is approximately 1 anyway."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As expected, the fit is not as good.  The value of $\\theta$ is correspondingly much larger.  Of course, the smaller vandermonde matrix $A$ is much better conditioned than before.\n",
      "\n",
      "Notice that now we expect the normal equations to be worse than a backward-stable algorithm by only about a factor of 3.  Let's check:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "SVD"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "U, S, Vstar = np.linalg.svd(A)\n",
      "xx = np.dot(Vstar.T,np.dot(U[:,:n].T,b)/S)\n",
      "print xx[-1]-x_ex[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Normal equations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xx = np.linalg.solve(np.dot(A.T,A),np.dot(A.T,b))\n",
      "print xx[-1]-x_ex[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}